{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#import torchvision.transforms as transforms\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.transforms.functional as TF\n",
    "from einops import rearrange\n",
    "from datasets import load_dataset\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "#from huggingface_hub import login\n",
    "#login(\"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\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import DetrImageProcessor\n",
    "from datasets import load_dataset\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection, DetrConfig\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize the processor\n",
    "processor = DetrImageProcessor.from_pretrained('facebook/detr-resnet-101')\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"detection-datasets/coco\", split='train[:1%]')\n",
    "\n",
    "def transform_data(sample):\n",
    "    image = sample['image']\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # Directly use image dimensions for verification\n",
    "    actual_width, actual_height = image.size\n",
    "    print(\"Actual dimensions:\", actual_width, actual_height)\n",
    "\n",
    "    # Convert bounding box format [x_min, y_min, width, height] to [x0, y0, x1, y1] without scaling\n",
    "    bboxes = []\n",
    "    for box in sample['objects']['bbox']:\n",
    "        x0 = box[0]\n",
    "        y0 = box[1]\n",
    "        x1 = x0 + box[2]\n",
    "        y1 = y0 + box[3]\n",
    "        print(f\"Box: ({x0}, {y0}, {x1}, {y1})\")  # Debug print\n",
    "        bboxes.append([x0, y0, x1, y1])\n",
    "    tensor_boxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "\n",
    "    labels = torch.tensor(sample['objects']['category'], dtype=torch.long)\n",
    "\n",
    "    return inputs, tensor_boxes, labels\n",
    "\n",
    "# Example usage for demonstration\n",
    "sample_data = dataset[0]\n",
    "print(sample_data)\n",
    "'''inputs, boxes, labels = transform_data(sample_data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def show_image_with_boxes(image, boxes):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    for box in boxes:\n",
    "        # Adjust rectangle drawing based on [x0, y0, x1, y1]\n",
    "        rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "inputs, boxes, labels = transform_data(sample_data)\n",
    "show_image_with_boxes(sample_data['image'], boxes.numpy())  # Ensure boxes are converted to numpy if needed\n",
    "\n",
    "'''\n",
    "\n",
    "# Helper modules\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten image patches into a sequence of embeddings\n",
    "        x = self.proj(x)\n",
    "        print(\"Shape after convolution:\", x.shape)\n",
    "\n",
    "        # We rearrange assuming the conv output is (batch, embed_dim, height // patch_size, width // patch_size)\n",
    "        # No need to pass patch_size dynamically to einops\n",
    "        return rearrange(x, 'b e h w -> b (h w) e')\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_layers, num_heads):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward=embed_dim*4)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_block = ConvBlock(in_channels, out_channels)\n",
    "        self.downsample = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x_downsampled = self.downsample(x)\n",
    "        return x, x_downsampled\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, skip_channels, kernel_size=2, stride=2)  # output channels match skip_channels\n",
    "        self.conv_block = ConvBlock(in_channels + skip_channels, out_channels)  # adjust the input channels of ConvBlock\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        print(f\"Input to Decoder Block: {x.shape}\")\n",
    "        x = self.upconv(x)\n",
    "        print(f\"x after upconv(x): {x.shape}\")\n",
    "        # Optional: Resize x to the size of skip before concatenating if there's a size mismatch\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            x = TF.resize(x, size=skip.shape[2:])\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        print(f\"Shape after concat in decoder: {x.shape}\")\n",
    "    \n",
    "        x = self.conv_block(x)\n",
    "        print(f\"Output of Decoder Block: {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HybridUNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, patch_size=16, embed_dim=768, num_heads=8, num_enc_layers=4, num_queries=100):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(patch_size, in_channels, embed_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(embed_dim, num_enc_layers, num_heads)\n",
    "\n",
    "        self.encoder1 = EncoderBlock(embed_dim, 64)\n",
    "        self.encoder2 = EncoderBlock(64, 128)\n",
    "        self.encoder3 = EncoderBlock(128, 256)\n",
    "        self.encoder4 = EncoderBlock(256, 512)\n",
    "\n",
    "        self.decoder1 = DecoderBlock(512, 256, 256)\n",
    "        self.decoder2 = DecoderBlock(256, 128, 128)\n",
    "        self.decoder3 = DecoderBlock(128, 64, 64)\n",
    "        self.decoder4 = DecoderBlock(64, embed_dim, 64)\n",
    "\n",
    "        self.classifier = nn.Conv2d(64, num_classes + 1, kernel_size=1)  # +1 for background class\n",
    "        self.bbox_predictor = nn.Conv2d(64, 4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = rearrange(x, 'b (h w) e -> b e h w', h=int(x.shape[1]**0.5))\n",
    "        enc_features = []\n",
    "        \n",
    "        # Encoding\n",
    "        skip, x = self.encoder1(x); enc_features.append(skip)\n",
    "        skip, x = self.encoder2(x); enc_features.append(skip)\n",
    "        skip, x = self.encoder3(x); enc_features.append(skip)\n",
    "        skip, x = self.encoder4(x); enc_features.append(skip)\n",
    "        \n",
    "        # Decoding\n",
    "        x = self.decoder1(x, enc_features.pop())\n",
    "        x = self.decoder2(x, enc_features.pop())\n",
    "        x = self.decoder3(x, enc_features.pop())\n",
    "        x = self.decoder4(x, enc_features.pop())\n",
    "\n",
    "        # Prediction Heads\n",
    "        logits = self.classifier(x)\n",
    "        bbox_outputs = self.bbox_predictor(x)\n",
    "        print(\"HYBRID UNETVIT OUTPUT:\")\n",
    "        print(f\"hybrid unet vit logits shape: {logits.shape}\")\n",
    "        print(f\"hybrid unet vit pred boxes shape: {bbox_outputs.shape}\")\n",
    "\n",
    "        return {'logits': logits, 'pred_boxes': bbox_outputs}\n",
    "\n",
    "\n",
    "\n",
    "def generate_segmentation_mask(anns, height, width):\n",
    "    \"\"\"\n",
    "    Generate a binary mask for segmentation from COCO annotations, assuming polygon annotations.\n",
    "    \"\"\"\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    for ann in anns:\n",
    "        if 'segmentation' in ann and isinstance(ann['segmentation'], list):  # Ensure there's a segmentation and it's a list of polygons\n",
    "            for seg in ann['segmentation']:\n",
    "                # Each 'seg' is a list of coordinates like [x1, y1, x2, y2, ..., xn, yn]\n",
    "                # It needs to be reshaped into a list of tuples [(x1, y1), (x2, y2), ..., (xn, yn)]\n",
    "                if len(seg) % 2 == 0:  # Ensure the list is even-length\n",
    "                    poly = np.array(seg).reshape((-1, 2))\n",
    "                    img = Image.new('L', (width, height), 0)\n",
    "                    ImageDraw.Draw(img).polygon(list(map(tuple, poly)), outline=1, fill=1)\n",
    "                    mask = np.logical_or(mask, np.array(img, dtype=bool))\n",
    "    \n",
    "    return mask.astype(np.uint8)\n",
    "\n",
    "\n",
    "def train_val_split(dataset, val_split=0.2):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(val_split * dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    return torch.utils.data.Subset(dataset, train_indices), torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "def validate_model(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            outputs_upsampled = F.interpolate(outputs, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            masks = torch.squeeze(masks, 1).long()\n",
    "\n",
    "            loss = loss_fn(outputs_upsampled, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(dataloader)\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class CustomCocoDataset(Dataset):\n",
    "    def __init__(self, dataset, target_size=(800, 800)):\n",
    "        self.dataset = dataset\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        image = sample['image']\n",
    "\n",
    "        # Ensure image is in RGB\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        # Resize image to target size using the correct torchvision function\n",
    "        image = TF.resize(image, self.target_size)\n",
    "\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        # Scale bounding boxes according to the new image size\n",
    "        scale_x = self.target_size[0] / sample['width']\n",
    "        scale_y = self.target_size[1] / sample['height']\n",
    "        bboxes = []\n",
    "        for box in sample['objects']['bbox']:\n",
    "            x0 = box[0] * scale_x\n",
    "            y0 = box[1] * scale_y\n",
    "            x1 = (box[0] + box[2]) * scale_x\n",
    "            y1 = (box[1] + box[3]) * scale_y\n",
    "            bboxes.append([x0, y0, x1, y1])\n",
    "        tensor_boxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "\n",
    "        labels = torch.tensor(sample['objects']['category'], dtype=torch.long)\n",
    "\n",
    "        return {'inputs': inputs['pixel_values'].squeeze(0), 'boxes': tensor_boxes, 'labels': labels}\n",
    "\n",
    "# Adjust collate function as needed:\n",
    "def collate_fn(batch):\n",
    "    inputs = torch.stack([item['inputs'] for item in batch])\n",
    "    max_boxes = max(len(item['boxes']) for item in batch)\n",
    "\n",
    "    padded_boxes = torch.zeros((len(batch), max_boxes, 4))\n",
    "    box_masks = torch.zeros((len(batch), max_boxes), dtype=torch.bool)\n",
    "    padded_labels = torch.zeros((len(batch), max_boxes), dtype=torch.long)  # Adjust if -1 creates problems\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        num_boxes = item['boxes'].shape[0]\n",
    "        padded_boxes[i, :num_boxes] = item['boxes']\n",
    "        padded_labels[i, :num_boxes] = item['labels']\n",
    "        box_masks[i, :num_boxes] = 1\n",
    "    print({'inputs': inputs.shape, 'boxes': padded_boxes.shape, 'labels': padded_labels.shape, 'box_masks': box_masks.shape})\n",
    "    return {'inputs': inputs, 'boxes': padded_boxes, 'labels': padded_labels, 'box_masks': box_masks}\n",
    "\n",
    "\n",
    "coco_dataset = CustomCocoDataset(dataset)\n",
    "\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset, val_dataset = train_val_split(dataset, val_split=0.2)\n",
    "\n",
    "# Now use this custom collate function in your DataLoader\n",
    "data_loader = DataLoader(coco_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "# Hyperparameters\n",
    "num_classes = 80\n",
    "lr = 1e-4  \n",
    "num_epochs = 20 \n",
    "# Example initialization\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = HybridUNet(in_channels=3, num_classes=80, patch_size=16, embed_dim=768, num_heads=8, num_enc_layers=4).to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()  # Or other suitable segmentation loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "\n",
    "# Loading a saved model\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# Define loss functions\n",
    "classification_loss_fn = nn.CrossEntropyLoss()\n",
    "bbox_regression_loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "# Define the training loop\n",
    "all_labels = torch.cat([item['labels'] for item in coco_dataset])\n",
    "unique_labels = torch.unique(all_labels)\n",
    "print(f\"Unique labels in the dataset: {unique_labels}\")\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        inputs = batch['inputs'].to(device)\n",
    "        boxes = batch['boxes'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        box_masks = batch['box_masks'].to(device)\n",
    "        \n",
    "        print(f\"inputs size: {inputs.shape}\")\n",
    "        print(f\"boxes size: {boxes.shape}\")\n",
    "        print(f\"labels size: {labels.shape}\")\n",
    "        print(f\"box_masks size: {box_masks.shape}\")\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        print(f\"Output logits size: {outputs['logits'].shape}\")\n",
    "        # Prepare targets for all non-padded areas\n",
    "        targets = []\n",
    "        for i in range(inputs.size(0)):\n",
    "\n",
    "            active_indices = box_masks[i]\n",
    "\n",
    "            active_boxes = boxes[i][active_indices]\n",
    "\n",
    "            active_labels = labels[i][active_indices]\n",
    "\n",
    "            targets.append({'labels': active_labels, 'boxes': active_boxes})\n",
    "        \n",
    "        # Compute the classification loss\n",
    "        logits = outputs['logits']\n",
    "        max_target_labels = max(len(target['labels']) for target in targets)\n",
    "        num_queries = logits.size(1)\n",
    "        padded_logits = torch.zeros((logits.size(0), num_queries, logits.size(2), logits.size(3)), device=device) # Include width dimension\n",
    "        for i, target in enumerate(targets):\n",
    "            padded_logits[i, :len(target['labels']), :, :] = logits[i, :len(target['labels']), :, :] # Add the width dimension\n",
    "        logits = padded_logits\n",
    "        print(f\"Padded logits size: {logits.shape}\")\n",
    "\n",
    "        target_labels = torch.cat([target['labels'] for target in targets], dim=0)\n",
    "\n",
    "        # Resize logits to match the number of target labels\n",
    "        num_target_labels = target_labels.size(0)\n",
    "        print(f\"num target labels: {num_target_labels}\")\n",
    "        logits_flat = logits.view(-1, logits.shape[-1])  # Shape: (batch_size * num_queries, num_classes)\n",
    "        print(f\"logits_flat: {logits_flat.shape}\")\n",
    "        \n",
    "        target_labels_flat = target_labels.view(-1)      # Shape: (batch_size * num_queries)\n",
    "        print(f\"target_labels_flat: {target_labels_flat.shape}\")\n",
    "\n",
    "        # Slice the logits tensor to match the size of the target_labels tensor\n",
    "        #logits_flat = logits_flat[:target_labels_flat.size(0)]\n",
    "        logits_flat = logits_flat[:num_target_labels]\n",
    "\n",
    "        print(f\"Shape of logits_flat: {logits_flat.shape}\")\n",
    "        print(f\"Shape of target_labels_flat: {target_labels_flat.shape}\")\n",
    "        print(f\"Unique values in target_labels_flat: {torch.unique(target_labels_flat)}\")\n",
    "\n",
    "        classification_loss = classification_loss_fn(logits_flat, target_labels_flat)\n",
    "\n",
    "        \n",
    "        # Compute the bbox regression loss\n",
    "        pred_boxes = outputs['pred_boxes']\n",
    "        target_boxes = torch.cat([target['boxes'] for target in targets])\n",
    "\n",
    "        # Compute the bbox regression loss\n",
    "        pred_boxes_flat = pred_boxes.view(-1, 4)  # Shape: (batch_size * num_queries, 4)\n",
    "        target_boxes_flat = target_boxes.view(-1, 4)  # Shape: (batch_size * num_queries, 4)\n",
    "\n",
    "        # Slice the pred_boxes_flat tensor to match the size of the target_boxes_flat tensor\n",
    "        pred_boxes_flat = pred_boxes_flat[:target_boxes_flat.size(0)]\n",
    "\n",
    "        # Compute L1 loss (smooth L1 loss) for bbox regression\n",
    "        loss_bbox = nn.SmoothL1Loss(reduction='sum')(pred_boxes_flat, target_boxes_flat)\n",
    "\n",
    "        #bbox_regression_loss = bbox_regression_loss_fn(pred_boxes, target_boxes)\n",
    "        \n",
    "        # Compute total loss\n",
    "        loss = classification_loss + loss_bbox\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Total Loss: {total_loss / len(data_loader)}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
